import osimport glob# Automatically detect and set the correct X11 display to prevent 'Connection refused' errorsdef _set_active_display():    if not os.path.exists('/tmp/.X11-unix'):        return    x11_sockets = glob.glob('/tmp/.X11-unix/X*')    if x11_sockets:        # Extract display number, e.g., from /tmp/.X11-unix/X1 to :1        active_disp = ':' + x11_sockets[0].replace('/tmp/.X11-unix/X', '')        current_disp = os.environ.get('DISPLAY')        # Apply if unset or if the currently requested display socket doesn't exist        if not current_disp or not os.path.exists(f"/tmp/.X11-unix/X{current_disp.strip(':')}"):            os.environ['DISPLAY'] = active_disp_set_active_display()import cv2import mediapipe as mpimport timefrom pynput.keyboard import Key, Controller# ==========================================# Initialize MediaPipe Hands (Tasks API)# ==========================================from mediapipe.tasks import python as mp_tasksfrom mediapipe.tasks.python.vision import hand_landmarker as hl_modulefrom mediapipe.tasks.python.vision import RunningModefrom threading import Threadmodel_path = os.path.join(os.path.dirname(__file__), "hand_landmarker.task")# Setup BaseOptions for Jetson Nano Inferencebase_options = mp_tasks.BaseOptions(    model_asset_path=model_path)options = hl_module.HandLandmarkerOptions(    base_options=base_options,    running_mode=RunningMode.VIDEO,    num_hands=1,    min_hand_detection_confidence=0.5, # Lowered from 0.7 for performance improvements    min_tracking_confidence=0.5,    min_hand_presence_confidence=0.5)landmarker = hl_module.HandLandmarker.create_from_options(options)HAND_CONNECTIONS = [    (0, 1), (1, 2), (2, 3), (3 , 4), (0, 5), (5, 6), (6, 7), (7, 8),    (5, 9), (9, 10), (10, 11), (11, 12), (9, 13), (13, 14), (14, 15),    (15, 16), (13, 17), (0, 17), (17, 18), (18, 19), (19, 20)]def map_landmarks(img, landmarks):    h, w, _ = img.shape    pts = {}    for i, lm in enumerate(landmarks):        lx, ly = int(lm.x * w), int(lm.y * h)        pts[i] = (lx, ly)        cv2.circle(img, (lx, ly), 5, (0, 0, 255), -1)    for p1, p2 in HAND_CONNECTIONS:        if p1 in pts and p2 in pts:            cv2.line(img, pts[p1], pts[p2], (0, 255, 0), 2)class DummyLandmarks:    def __init__(self, landmarks):        self.landmark = landmarks# ==========================================# Threaded Camera Capturer (For zero I/O Latency)# ==========================================class VideoStream:    def __init__(self, src=0, width=480, height=360):        self.stream = cv2.VideoCapture(src)        self.stream.set(cv2.CAP_PROP_FRAME_WIDTH, width)        self.stream.set(cv2.CAP_PROP_FRAME_HEIGHT, height)        (self.grabbed, self.frame) = self.stream.read()        self.stopped = False    def start(self):        # Start the thread to read frames from the video stream        Thread(target=self.update, args=(), daemon=True).start()        return self    def update(self):        # Keep looping infinitely until the thread is stopped        while not self.stopped:            (self.grabbed, self.frame) = self.stream.read()    def read(self):        # Return the frame most recently read        return self.grabbed, self.frame    def stop(self):        self.stopped = True        self.stream.release()# ==========================================# Initialize Keyboard Controller# ==========================================keyboard = Controller()# ==========================================# Gesture State Variables# ==========================================current_gesture = Nonegesture_frames = 0GESTURE_CONFIDENCE_FRAMES = 15  # Frames needed to confirm a gesture (debouncing)# Cooldown to avoid spamming the same action multiple timescooldown_counter = 0def toggle_play_pause():    keyboard.press(Key.space)    keyboard.release(Key.space)    print("Action Triggered: Play/Pause (Space)")def volume_up():    keyboard.press(Key.up)    keyboard.release(Key.up)    print("Action Triggered: Volume Up (Up Arrow)")def volume_down():    keyboard.press(Key.down)    keyboard.release(Key.down)    print("Action Triggered: Volume Down (Down Arrow)")def mute():    keyboard.press('m')    keyboard.release('m')    print("Action Triggered: Mute ('m')")def next_track():    keyboard.press('n')    keyboard.release('n')    print("Action Triggered: Next Track ('n')")def previous_track():    keyboard.press('p')    keyboard.release('p')    print("Action Triggered: Previous Track ('p')")def seek_forward():    keyboard.press(Key.right)    keyboard.release(Key.right)    print("Action Triggered: Seek Forward (Alt + Right Arrow)")def seek_backward():    keyboard.press(Key.left)    keyboard.release(Key.left)    print("Action Triggered: Seek Backward (Alt + Left Arrow)")def execute_action(gesture):    """    Map recognized gestures to VLC shortcut actions:    - Fist (0 fingers): Mute ('m')    - 1 Finger (Index): Next Track ('n')    - 2 Fingers: Previous Track ('p')    - 3 Fingers: Seek Forward  ('Alt + Right')    - 4 Fingers: Seek Backward ('Alt + Left')    - 5 Fingers: Play/Pause ('Space')    - Thumb Up: Volume Up ('Up Arrow')    - Thumb Down: Volume Down ('Down Arrow')    """    if gesture == "FIST":        mute()    elif gesture == "ONE_FINGER":        next_track()    elif gesture == "TWO_FINGERS":        previous_track()    elif gesture == "THREE_FINGERS":        seek_forward()    elif gesture == "FOUR_FINGERS":        seek_backward()    elif gesture == "FIVE_FINGERS":        toggle_play_pause()    elif gesture == "THUMB_UP":        volume_up()    elif gesture == "THUMB_DOWN":        volume_down()def classify_gesture(hand_landmarks, handedness):    """    Identify specific hand gestures (Fist, Thumb Up, Thumb Down, 1-5 fingers).    """    fingers = []       # MediaPipe finger tip/pip indices    # Thumb: 4 (tip), 3 (ip), 2 (mcp)    # Index: 8 (tip), 6 (pip)    # Middle: 12 (tip), 10 (pip)    # Ring: 16 (tip), 14 (pip)    # Pinky: 20 (tip), 18 (pip)    tip_ids = [4, 8, 12, 16, 20]    pip_ids = [3, 6, 10, 14, 18]       # 1. Thumb (Tip X vs IP X considering Handedness)    if handedness == "Right":        fingers.append(hand_landmarks.landmark[tip_ids[0]].x < hand_landmarks.landmark[pip_ids[0]].x)    else: # Left hand        fingers.append(hand_landmarks.landmark[tip_ids[0]].x > hand_landmarks.landmark[pip_ids[0]].x)               # 2. Four Fingers (Tip Y vs PIP Y)    for i in range(1, 5):        fingers.append(hand_landmarks.landmark[tip_ids[i]].y < hand_landmarks.landmark[pip_ids[i]].y)           num_fingers = fingers.count(True)    # Specific Gesture Logic    if num_fingers == 0:        return "FIST"    elif num_fingers == 5:        return "FIVE_FINGERS"           # Thumb Up / Down detection (Only thumb extended)    if fingers[0] and not any(fingers[1:]):        # Compare Thumb Tip Y against Index MCP Y (landmark 5)        if hand_landmarks.landmark[4].y < hand_landmarks.landmark[5].y:            return "THUMB_UP"        else:            return "THUMB_DOWN"               if num_fingers == 1 and fingers[1]:        return "ONE_FINGER"    elif num_fingers == 2 and fingers[1] and fingers[2]:        return "TWO_FINGERS"    elif num_fingers == 3 and fingers[1] and fingers[2] and fingers[3]:        return "THREE_FINGERS"    elif num_fingers == 4 and fingers[1] and fingers[2] and fingers[3] and fingers[4]:        return "FOUR_FINGERS"           return Nonedef main():    print("Starting Optimized Touchless HCI System on Jetson Nano...")       # Open threaded webcam for asynchronous I/O capture (boosts FPS)    cap = VideoStream(src=0, width=480, height=360).start()        # Warmup camera    time.sleep(1.0)    p_time = 0    # Process only every Nth frame, or set to 1 to process all.    # CRITICAL JETSON FIX: Set to 2 or 3! This forces inference to happen on alternate     # frames only. The webcam feed will still stream smoothly at 15+ FPS to the screen,     # but the CPU won't lock up trying to run ML models 30 times a second.    frame_skip = 3    f_cnt = 0        global current_gesture, gesture_frames, cooldown_counter    # Track gesture state across skipped frames    last_detected_gesture = None    try:        while True:            success, img = cap.read()            if not success or img is None:                print("Failed to read from camera. Retrying...")                time.sleep(0.1)                continue                           f_cnt = f_cnt + 1            # Flip horizontally for a natural selfie-view            img = cv2.flip(img, 1)                       detected_gesture = last_detected_gesture            # Only process alternating frames to ensure 15+ FPS display responsiveness and lower latency            if f_cnt % frame_skip == 0:                # Convert BGR (OpenCV format) to RGB (MediaPipe requirement)                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                               try:                    # Process frame with Tasks API in VIDEO mode for lightning fast tracking                    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)                    timestamp_ms = int(time.time() * 1000)                    results = landmarker.detect_for_video(mp_image, timestamp_ms)                                       detected_gesture = None                                       if results.hand_landmarks:                        for hand_landmarks, handedness_info in zip(results.hand_landmarks, results.handedness):                            # Draw skeletal connections                            map_landmarks(img, hand_landmarks)                                                       # Detect left/right hand                            handedness = handedness_info[0].category_name                                                       # Compute gesture                            # Wrap hand_landmarks in DummyLandmarks to match the existing logic                            detected_gesture = classify_gesture(DummyLandmarks(hand_landmarks), handedness)                                               last_detected_gesture = detected_gesture                except Exception as e:                    print(f"Inference error handling: {e}")                    # If GPU delegate fails on older Jetpack, fallback or skip frame handling happens here organically                    detected_gesture = None                    last_detected_gesture = None                        # --- Always Render Visual Info (Eliminates Flickering) ---            display_text = detected_gesture if detected_gesture else "None"            cv2.putText(img, f'Gesture: {display_text}', (10, 80),                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)                                           # --- Debouncing & Cooldown Logic ---            if cooldown_counter > 0:                cooldown_counter -= 1                cv2.putText(img, 'Cooldown...', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)            else:                if detected_gesture is not None:                    if detected_gesture == current_gesture:                        gesture_frames += 1                        # If a gesture is held for long enough, execute                        if gesture_frames >= GESTURE_CONFIDENCE_FRAMES:                            execute_action(current_gesture)                            gesture_frames = 0                                                       # Different cooldown times depending on the action                            if current_gesture in ["FIST", "ONE_FINGER", "TWO_FINGERS", "FIVE_FINGERS"]:                                # Toggle/Switch actions need longer cooldown to avoid repeating instantly                                cooldown_counter = 60                            else:                                # Continuous actions (Volume, Seeking)                                cooldown_counter = 15                    else:                        current_gesture = detected_gesture                        gesture_frames = 1                else:                    # Reset if no hand is detected or gesture unrecognized                    current_gesture = None                    gesture_frames = 0                               # --- Performance Metrics ---            c_time = time.time()            fps = 1 / (c_time - p_time) if (c_time - p_time) > 0 else 0            p_time = c_time            cv2.putText(img, f'FPS: {int(fps)}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)                       # Show output feed            cv2.imshow("Jetson Nano Touchless Media Control", img)                       # Break the loop on 'q' press            if cv2.waitKey(1) & 0xFF == ord('q'):                break                   finally:        cap.stop()        cv2.destroyAllWindows()if __name__ == "__main__":    main()